{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection Criteria (Calculating AIC, BIC and MDL )\n",
    "    \n",
    "Example : Linear Regression\n",
    "\n",
    "- Using a test problem and fiting a linear regression model, then evaluating the model using the AIC and BIC metrics.\n",
    "- The problem will have two input variables and require the prediction of a target numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# generate dataset\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# define and fit the model on all data\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, The number of  parameters in the model is calculated.\n",
    "- Expected : 3(two coefficients and one intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3\n"
     ]
    }
   ],
   "source": [
    "# number of parameters\n",
    "num_params = len(model.coef_) + 1\n",
    "print('Number of parameters: %d' % (num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function for a linear regression model can be shown to be identical to the least squares function; therefore, we can estimate the maximum likelihood of the model via the mean squared error metric.\n",
    "\n",
    "1. the model can be used to estimate an outcome for each example in the training dataset.\n",
    "2. the mean_squared_error() scikit-learn function can be used to calculate the mean squared error for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# predict the training set\n",
    "yhat = model.predict(X)\n",
    "# calculate the error\n",
    "mse = mean_squared_error(y, yhat)\n",
    "print('MSE: %.3f' % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reporting**\n",
    "- Number of Parameters: 3\n",
    "- MSE value : 0.008\n",
    "\n",
    "The specific MSE value may vary running each time, due to the stochastic nature of the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC\n",
    "\n",
    "The AIC calculation for an ordinary least squares linear regression model can be calculated as follows (taken from “A New Look At The Statistical Identification Model“,  1974.):\n",
    "\n",
    "$AIC = n * LL + 2 * k$\n",
    "\n",
    "Where $n$ is the number of examples in the training dataset, $LL$ is the log-likelihood for the model using the natural logarithm (e.g. the log of the MSE), and $k$ is the number of parameters in the model.\n",
    "\n",
    "#### Defining AIC function\n",
    "\n",
    "- Function : calculate_aic() \n",
    "- Arguments : n(no. of examples), the raw mean squared error (mse), and k(no. of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC: -477.529\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "# calculate aic for regression\n",
    "def calculate_aic(n, mse, num_params):\n",
    "    aic = n * log(mse) + 2 * num_params\n",
    "    return aic\n",
    "\n",
    "aic = calculate_aic(len(y), mse, num_params)\n",
    "print('AIC: %.3f' % aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This value can be minimized in order to choose better models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIC\n",
    "\n",
    "*The same example can be explored to calculate BIC* \n",
    "\n",
    "The BIC calculation for an ordinary least squares linear regression model can be calculated as follows (taken from [wikipedia-BIC-GaussianSpecialCase](https://en.wikipedia.org/wiki/Bayesian_information_criterion#Gaussian_special_case)):\n",
    "\n",
    "$BIC = n * LL + k * log(n)$\n",
    "\n",
    "Where $n$ is the number of examples in the training dataset, $LL$ is the log-likelihood for the model using the natural logarithm (e.g. log of the mean squared error), and $k$ is the number of parameters in the model, and $log()$ is the natural logarithm.\n",
    "\n",
    "\n",
    "#### Defining BIC function\n",
    "\n",
    "- Function : calculate_bic() \n",
    "- Arguments : n(no. of examples), the raw mean squared error (mse), and k(no. of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC: -469.713\n"
     ]
    }
   ],
   "source": [
    "# calculate bic for regression\n",
    "def calculate_bic(n, mse, num_params):\n",
    "    bic = n * log(mse) + num_params * log(n)\n",
    "    return bic\n",
    "\n",
    "bic = calculate_bic(len(y), mse, num_params)\n",
    "print('BIC: %.3f' % bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again :* the results may vary given the stochastic nature of the learning algorithm.\n",
    "\n",
    "In this case, the BIC is reported to be a value of about -469.713, which is very close to the AIC value of -477.529. Again, this value can be minimized in order to choose better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDL\n",
    "\n",
    "The Minimum Description Length (MDL) principle recommends choosing the hypothesis that minimizes the sum of following two description lengths.\n",
    "\n",
    "$MDL = L(h) + L(D | h)$\n",
    "\n",
    "Where $h$ is the model, $D$ is the predictions made by the model, $L(h)$ is the number of bits required to represent the model, and $L(D | h)$ is the number of bits required to represent the predictions from the model on the training dataset\n",
    "\n",
    "> *minimum number of bits, or the minimum of the sum of the number of bits required to represent the data and the model.(i.e. The model with the lowest MDL) is selected*\n",
    "\n",
    "\n",
    "Since the score needs to be minimized, the number of bits required to encode $(D | h)$ and the number of bits required to encode $(h)$ can be calculated as the negative log-likelihood; the negative log-likelihood of the model parameters (theta) and the negative log-likelihood of the target values (y) given the input values (X) and the model parameters (theta).\n",
    "\n",
    "$MDL = -log(P(\\theta)) – log(P(y | X, \\theta))$\n",
    "\n",
    "#### Defining MDL function\n",
    "\n",
    "- Function : calculate_mdl() \n",
    "- Arguments : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps \n",
    "\n",
    "# 1. calculating negative log likelihood of model parameter\n",
    "\n",
    "# 2. calculating negative log likelihood of y given X and theta\n",
    "\n",
    "## https://github.com/maxpumperla/entropy-mdlp/blob/master/entropymdlp/mdlp.py(- wanna try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
